{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cardiovascular-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import *\n",
    "from utils.utils import *\n",
    "from models import *\n",
    "from utils import torch_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "specified-daisy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='Quadro K620', total_memory=2048MB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch_utils.select_device(device='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "iraqi-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "source=r'C:\\Development\\dev_tools\\tree-detector-yolo\\data\\t1f_22.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "limiting-october",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 225 layers, 6.25733e+07 parameters, 6.25733e+07 gradients\n"
     ]
    }
   ],
   "source": [
    "cfg=r'C:\\\\Users\\\\lerryw\\\\Google Drive\\\\acacia_dataset\\\\cfg\\\\yolov3-spp-1cls.cfg'\n",
    "weights=r'C:\\\\Users\\\\lerryw\\\\Google Drive\\\\acacia_dataset\\\\weights\\\\last_1cls_1300.pt'\n",
    "\n",
    "# Initialize model\n",
    "model = Darknet(cfg, 416)\n",
    "\n",
    "if weights.endswith('.pt'):  # pytorch format\n",
    "    model.load_state_dict(torch.load(weights, map_location=device)['model'])\n",
    "else:  # darknet format\n",
    "    load_darknet_weights(model, weights)\n",
    "    \n",
    "model.to(device).eval()\n",
    "# Half precision\n",
    "half=True\n",
    "half = half and device.type != 'cpu'  # half precision only supported on CUDA\n",
    "if half:\n",
    "    model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ignored-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = LoadImages(source, img_size=1024)\n",
    "dataset=ImageFolder(source, batch_size=1, img_size=416)\n",
    "dataset2=LoadImages(source,img_size=416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "color-seventh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (['C:\\\\Development\\\\dev_tools\\\\tree-detector-yolo\\\\data\\\\t1f_22.tif'], (3, 666, 666))\n"
     ]
    }
   ],
   "source": [
    "## Test dataset1\n",
    "for batch_i, (img_paths, img) in enumerate(dataset):\n",
    "    print(batch_i, (img_paths, img.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "technological-checklist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 C:\\Development\\dev_tools\\tree-detector-yolo\\data\\t1f_22.tif: C:\\Development\\dev_tools\\tree-detector-yolo\\data\\t1f_22.tif (3, 416, 416) (666, 666, 3) None\n",
      "ni: 2 nj: 1\n",
      "row 0/2: 0 -413 3 0 416\n",
      "row 1/2: 0 -413 3 250 666\n"
     ]
    }
   ],
   "source": [
    "for path, img, im0s, vid_cap in dataset2:\n",
    "    print(path, img.shape, im0s.shape, vid_cap)\n",
    "    \n",
    "    preds = []\n",
    "    length = 416\n",
    "    ni = int(math.ceil(im0s.shape[1] / length))  # up-down\n",
    "    nj = int(math.ceil(im0s.shape[2] / length))  # left-right\n",
    "    print(\"ni:\",ni,\"nj:\",nj)\n",
    "    for i in range(ni):\n",
    "        print('row %g/%g: ' % (i, ni), end='')\n",
    "        for j in range(nj):\n",
    "            print('%g ' % j, end='', flush=True)\n",
    "            \n",
    "            y2 = min((i + 1) * length, im0s.shape[1])\n",
    "            y1 = y2 - length\n",
    "            x2 = min((j + 1) * length, im0s.shape[2])\n",
    "            x1 = x2 - length\n",
    "            print(x1,x2,y1,y2)\n",
    "#             with torch.no_grad():\n",
    "#                 chip = torch.from_numpy(img[:, y1:y2, x1:x2]).unsqueeze(0).to(device)\n",
    "#                 pred = model(chip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-float",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "departmental-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name=r'C:\\\\Users\\\\lerryw\\\\Google Drive\\\\acacia_dataset\\\\acacia.names'\n",
    "names = load_classes(class_name)\n",
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "img = torch.zeros((1, 3, 416, 416), device=device)\n",
    "_ = model(img.half() if half else img.float()) if device.type != 'cpu' else None\n",
    "total_predicted_box = list()\n",
    "class_list = list()\n",
    "min_wh, max_wh = 2, 4096 \n",
    "for batch_i, (path, img) in enumerate(dataset):\n",
    "    print('\\n', path, img.shape, end=' ')\n",
    "    img_ud = np.ascontiguousarray(np.flip(img, axis=1))\n",
    "    img_lr = np.ascontiguousarray(np.flip(img, axis=2))\n",
    "    \n",
    "    preds = []\n",
    "    length = 416\n",
    "    ni = int(math.ceil(img.shape[1] / length))  # up-down\n",
    "    nj = int(math.ceil(img.shape[2] / length))  # left-right\n",
    "    \n",
    "    # scan image for each row based on slicing size\n",
    "    for i in range(ni):  # for i in range(ni - 1):\n",
    "        print('\\nrow %g/%g: ' % (i, ni), end='')\n",
    "        for j in range(nj):  # for j in range(nj if i==0 else nj - 1):\n",
    "            print('\\n%g ' % j, end='', flush=True)\n",
    "\n",
    "            # forward scan\n",
    "            y2 = min((i + 1) * length, img.shape[1])\n",
    "            y1 = y2 - length\n",
    "            x2 = min((j + 1) * length, img.shape[2])\n",
    "            x1 = x2 - length\n",
    "#             print(y2,y1,x2,x1,'\\n')\n",
    "            with torch.no_grad():\n",
    "                # Normal orientation\n",
    "                chip = torch.from_numpy(img[:, y1:y2, x1:x2]).unsqueeze(0).to(device)\n",
    "                chip = chip.half() if half else chip.float()  # uint8 to fp16/32\n",
    "                chip /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "                if chip.ndimension() == 3:\n",
    "                    chip = chip.unsqueeze(0)\n",
    "                    \n",
    "                pred = model(chip, augment=True)[0]\n",
    "\n",
    "                if half:\n",
    "                    pred = pred.float()\n",
    "                    \n",
    "                pred = non_max_suppression(pred, 0.7, 0.4,\n",
    "                                   multi_label=False, classes=None, agnostic=True)\n",
    "                \n",
    "                for i, det in enumerate(pred):\n",
    "                    print(len(det))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gross-extreme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ayam.tif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "source=r'C:\\\\Users\\\\lerryw\\\\Google Drive\\\\acacia_dataset\\\\ayam.tif'\n",
    "base = os.path.basename(source)\n",
    "name =  os.path.splitext(base)\n",
    "filename = os.path.splitext(os.path.basename(source))[0]\n",
    "print(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-equipment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
