{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "robust-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import *\n",
    "from utils.utils import *\n",
    "from models import *\n",
    "from utils import torch_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "democratic-friend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='Quadro K620', total_memory=2048MB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch_utils.select_device(device='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "indoor-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "source=r'C:\\Development\\dev_tools\\tree-detector-yolo\\data\\t1f_22.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beautiful-mitchell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 225 layers, 6.25733e+07 parameters, 6.25733e+07 gradients\n"
     ]
    }
   ],
   "source": [
    "cfg=r'C:\\\\Users\\\\lerryw\\\\Google Drive\\\\acacia_dataset\\\\cfg\\\\yolov3-spp-1cls.cfg'\n",
    "weights=r'C:\\\\Users\\\\lerryw\\\\Google Drive\\\\acacia_dataset\\\\weights\\\\last_1cls_1300.pt'\n",
    "\n",
    "# Initialize model\n",
    "model = Darknet(cfg, 416)\n",
    "\n",
    "if weights.endswith('.pt'):  # pytorch format\n",
    "    model.load_state_dict(torch.load(weights, map_location=device)['model'])\n",
    "else:  # darknet format\n",
    "    load_darknet_weights(model, weights)\n",
    "    \n",
    "model.to(device).eval()\n",
    "# Half precision\n",
    "half=True\n",
    "half = half and device.type != 'cpu'  # half precision only supported on CUDA\n",
    "if half:\n",
    "    model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "smart-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = LoadImages(source, img_size=1024)\n",
    "dataset=ImageFolder(source, batch_size=1, img_size=416)\n",
    "dataset2=LoadImages(source,img_size=416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lightweight-maintenance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (['C:\\\\Development\\\\dev_tools\\\\tree-detector-yolo\\\\data\\\\t1f_22.tif'], (3, 666, 666))\n"
     ]
    }
   ],
   "source": [
    "## Test dataset1\n",
    "for batch_i, (img_paths, img) in enumerate(dataset):\n",
    "    print(batch_i, (img_paths, img.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "surprised-sunday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 C:\\Development\\dev_tools\\tree-detector-yolo\\data\\t1f_22.tif: C:\\Development\\dev_tools\\tree-detector-yolo\\data\\t1f_22.tif (3, 416, 416) (666, 666, 3) None\n",
      "ni: 2 nj: 1\n",
      "row 0/2: 0 -413 3 0 416\n",
      "row 1/2: 0 -413 3 250 666\n"
     ]
    }
   ],
   "source": [
    "for path, img, im0s, vid_cap in dataset2:\n",
    "    print(path, img.shape, im0s.shape, vid_cap)\n",
    "    \n",
    "    preds = []\n",
    "    length = 416\n",
    "    ni = int(math.ceil(im0s.shape[1] / length))  # up-down\n",
    "    nj = int(math.ceil(im0s.shape[2] / length))  # left-right\n",
    "    print(\"ni:\",ni,\"nj:\",nj)\n",
    "    for i in range(ni):\n",
    "        print('row %g/%g: ' % (i, ni), end='')\n",
    "        for j in range(nj):\n",
    "            print('%g ' % j, end='', flush=True)\n",
    "            \n",
    "            y2 = min((i + 1) * length, im0s.shape[1])\n",
    "            y1 = y2 - length\n",
    "            x2 = min((j + 1) * length, im0s.shape[2])\n",
    "            x1 = x2 - length\n",
    "            print(x1,x2,y1,y2)\n",
    "#             with torch.no_grad():\n",
    "#                 chip = torch.from_numpy(img[:, y1:y2, x1:x2]).unsqueeze(0).to(device)\n",
    "#                 pred = model(chip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-printer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "formal-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name=r'C:\\\\Users\\\\lerryw\\\\Google Drive\\\\acacia_dataset\\\\acacia.names'\n",
    "names = load_classes(class_name)\n",
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "img = torch.zeros((1, 3, 416, 416), device=device)\n",
    "_ = model(img.half() if half else img.float()) if device.type != 'cpu' else None\n",
    "total_predicted_box = list()\n",
    "class_list = list()\n",
    "min_wh, max_wh = 2, 4096 \n",
    "for batch_i, (path, img) in enumerate(dataset):\n",
    "    print('\\n', path, img.shape, end=' ')\n",
    "    img_ud = np.ascontiguousarray(np.flip(img, axis=1))\n",
    "    img_lr = np.ascontiguousarray(np.flip(img, axis=2))\n",
    "    \n",
    "    preds = []\n",
    "    length = 416\n",
    "    ni = int(math.ceil(img.shape[1] / length))  # up-down\n",
    "    nj = int(math.ceil(img.shape[2] / length))  # left-right\n",
    "    \n",
    "    # scan image for each row based on slicing size\n",
    "    for i in range(ni):  # for i in range(ni - 1):\n",
    "        print('\\nrow %g/%g: ' % (i, ni), end='')\n",
    "        for j in range(nj):  # for j in range(nj if i==0 else nj - 1):\n",
    "            print('\\n%g ' % j, end='', flush=True)\n",
    "\n",
    "            # forward scan\n",
    "            y2 = min((i + 1) * length, img.shape[1])\n",
    "            y1 = y2 - length\n",
    "            x2 = min((j + 1) * length, img.shape[2])\n",
    "            x1 = x2 - length\n",
    "#             print(y2,y1,x2,x1,'\\n')\n",
    "            with torch.no_grad():\n",
    "                # Normal orientation\n",
    "                chip = torch.from_numpy(img[:, y1:y2, x1:x2]).unsqueeze(0).to(device)\n",
    "                chip = chip.half() if half else chip.float()  # uint8 to fp16/32\n",
    "                chip /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "                if chip.ndimension() == 3:\n",
    "                    chip = chip.unsqueeze(0)\n",
    "                    \n",
    "                pred = model(chip, augment=True)[0]\n",
    "\n",
    "                if half:\n",
    "                    pred = pred.float()\n",
    "                    \n",
    "                pred = non_max_suppression(pred, 0.7, 0.4,\n",
    "                                   multi_label=False, classes=None, agnostic=True)\n",
    "                \n",
    "                for i, det in enumerate(pred):\n",
    "                    print(len(det))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "opposite-tomato",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ayam.tif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "source=r'C:\\\\Users\\\\lerryw\\\\Google Drive\\\\acacia_dataset\\\\ayam.tif'\n",
    "base = os.path.basename(source)\n",
    "name =  os.path.splitext(base)\n",
    "filename = os.path.splitext(os.path.basename(source))[0]\n",
    "print(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "objective-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "surprised-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets(imagedirs, train_txt='./train.txt', test_txt='./test.txt', train_size=0.75):\n",
    "    \"\"\"\n",
    "    INSTRUCTION:\n",
    "    \n",
    "    imagedirs: Directory to images files, will read file in sub-folder too\n",
    "    train_txt: Save train dataset to directory\n",
    "    train_txt: Save test dataset to directory\n",
    "    train_size: Split data size\n",
    "    \"\"\"\n",
    "    if os.path.exists(imagedirs):\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception('Not a valid directory')\n",
    "            \n",
    "    valid_images = [\".jpg\",\".png\",\".tif\"]\n",
    "    data_list = []\n",
    "    for root, directories, files in os.walk(imagedirs, topdown=False):\n",
    "        for f in files:\n",
    "            title, ext = os.path.splitext(os.path.basename(f))\n",
    "            if ext.lower() in valid_images:\n",
    "                data_list.append(os.path.join(root, f))\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "    train, test = train_test_split(data_list, train_size = train_size)\n",
    "    \n",
    "    with open(train_txt, 'w') as filehandle:\n",
    "        for x in train:\n",
    "            filehandle.write('%s\\n' % x)\n",
    "        \n",
    "    with open(test_txt, 'w') as filehandle:\n",
    "        for x in test:\n",
    "            filehandle.write('%s\\n' % x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "treated-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir=r'C:\\Map\\Deep_learning\\digital_globe\\011802594010_01\\pansharp_tiles'\n",
    "cap = r'E:\\asdas\\kkkkkkkkkkd'\n",
    "\n",
    "datasets(filedir, train_txt='./train.txt', test_txt='./test.txt', train_size=0.75)\n",
    "\n",
    "# train, test = train_test_split(df, train_size = 0.35)\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# with open('train0.txt', 'wb') as outfile:\n",
    "#     for data_slice in data:\n",
    "#         np.savetxt(outfile, data_slice, fmt='%4.1f', newline='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "presidential-advocate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function datasets in module __main__:\n",
      "\n",
      "datasets(imagedirs, train_txt='./train.txt', test_txt='./test.txt', train_size=0.75)\n",
      "    INSTRUCTION:\n",
      "    \n",
      "    imagedirs: Directory to images files, will read file in sub-folder too\n",
      "    train_txt: Save train dataset to directory\n",
      "    train_txt: Save test dataset to directory\n",
      "    train_size: Split data size\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "secondary-tunisia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465 864\n"
     ]
    }
   ],
   "source": [
    "print(len(train),len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, directories, files in os.walk(filedir, topdown=False):\n",
    "#     print(len(files))\n",
    "    for name in files:\n",
    "        title, ext = os.path.splitext(os.path.basename(name))\n",
    "#         print(os.path.join(root, name))\n",
    "        if ext == '.tif' or ext == '.jpg' or ext == '.png':\n",
    "            print(os.path.join(root, name))\n",
    "#     for name in directories:\n",
    "#         print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "anticipated-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import train_test_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "skilled-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_txt(filedir, train_txt='./train000.txt', test_txt='./test000.txt', train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-fountain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
